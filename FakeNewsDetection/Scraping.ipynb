{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "\n",
    "Since the datasets obtained through API do not include the text of news, but they include web links for them, scraping is needed to get the texts. The code for scraping has two functions: *scraping* and *process_csv_file*.\n",
    "\n",
    "*scraping:* The function attempts to scrape text using the web links.\n",
    "\n",
    "*process_csv_file:* Using scraping function, this function scrapes texts of news through links for a given csv file and adds the scraped texts in a column named \"text\". Only news in specific topics is scraped.\n",
    "\n",
    "Main code ensures each csv file in a given directory is scraped. Since the datasets obtained through API was in a format that is a csv per day, this process was needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "def scraping(url):\n",
    "    # Non-User Agent\n",
    "    try:\n",
    "        cntents = urllib.request.urlopen(url).read()\n",
    "    except:\n",
    "        print(\"HTTP Error 403: Forbidden\")\n",
    "    import requests\n",
    "    headers = {\n",
    "        'user-agent': '--',\n",
    "    }\n",
    "    # User Agent\n",
    "    r = requests.get(url,headers=headers)\n",
    "    print(r.text)\n",
    "    from lxml import html\n",
    "    # Parse the HTML\n",
    "    tree = html.fromstring(r.text)\n",
    "    # Extract text from all paragraphs\n",
    "    paragraphs = tree.xpath('//p//text()')\n",
    "    paragraph=\" \".join(paragraphs)\n",
    "    return paragraph\n",
    "\n",
    "# Define a function to process a single CSV file\n",
    "def process_csv_file(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    desired_sections = [\"Politics\", \"News\", \"UK news\", \"World news\", \"Australia news\", \"US news\", \"Global\", \"Global development\"]\n",
    "    df = df[df[\"sectionName\"].isin(desired_sections)]\n",
    "    df\n",
    "\n",
    "    # Replace the following line with your own code for scraping news text from the URLs\n",
    "    # For example, you can use your existing scraping function here\n",
    "    new_column_values=[]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['webUrl']\n",
    "        new_column_values.append(scraping(url))\n",
    "    df['text']=new_column_values\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "csv_directory=\"tempdata/articles/deneme\"\n",
    "#\n",
    "# Process each CSV file in the directory\n",
    "for filename in os.listdir(csv_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file = os.path.join(csv_directory, filename)\n",
    "        processed_df = process_csv_file(csv_file)\n",
    "        # Save the updated DataFrame back to the same CSV file\n",
    "        processed_df.to_csv(csv_file, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
