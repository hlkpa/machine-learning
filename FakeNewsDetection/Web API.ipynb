{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20a258f-75ad-41df-9edf-5228570b78a2",
   "metadata": {},
   "source": [
    "## Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175a17d-d880-4a4c-a9a5-a78b57e471b8",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The proliferation of fake news in today's digital age has become a significant concern. In response to this, various techniques and methodologies have been developed to detect and combat fake news. In this project, we aim to leverage data mining techniques to gather news articles from The Guardian and subsequently analyze them for fake news detection.\n",
    "\n",
    "### Data Collection\n",
    "We utilized The Guardian's API to collect news articles for analysis. The data collection process involved the following steps:\n",
    "\n",
    "1. API Access: Access to The Guardian's API was established using a unique API key.\n",
    "2. Date Range Specification: We specified a date range from January 1, 2021, to January 15, 2021, for data collection.\n",
    "3. Article Retrieval: Using the API, we retrieved articles published within the specified date range. The articles were retrieved with additional metadata including the article's ID, title, URL, section name, and publication date.\n",
    "4. Data Storage: The collected data was stored in CSV format for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdbf3b3-8516-4dc2-8c67-83e4fee844d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: tempdata/articles/2021-01-01.csv\n",
      "File already exists: tempdata/articles/2021-01-02.csv\n",
      "File already exists: tempdata/articles/2021-01-03.csv\n",
      "File already exists: tempdata/articles/2021-01-04.csv\n",
      "File already exists: tempdata/articles/2021-01-05.csv\n",
      "File already exists: tempdata/articles/2021-01-06.csv\n",
      "File already exists: tempdata/articles/2021-01-07.csv\n",
      "File already exists: tempdata/articles/2021-01-08.csv\n",
      "File already exists: tempdata/articles/2021-01-09.csv\n",
      "File already exists: tempdata/articles/2021-01-10.csv\n",
      "File already exists: tempdata/articles/2021-01-11.csv\n",
      "File already exists: tempdata/articles/2021-01-12.csv\n",
      "File already exists: tempdata/articles/2021-01-13.csv\n",
      "File already exists: tempdata/articles/2021-01-14.csv\n",
      "File already exists: tempdata/articles/2021-01-15.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "from datetime import date, timedelta\n",
    "\n",
    "ARTICLES_DIR = join('tempdata', 'articles')\n",
    "makedirs(ARTICLES_DIR, exist_ok=True)\n",
    "\n",
    "MY_API_KEY = open(r\"Guardian_API.txt\").read().strip()\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "my_params = {\n",
    "    'from-date': \"\",\n",
    "    'to-date': \"\",\n",
    "    'order-by': \"newest\",\n",
    "    'show-fields': 'all',\n",
    "    'page-size': 200,\n",
    "    'api-key': MY_API_KEY\n",
    "}\n",
    "\n",
    "start_date = date(2021, 1, 1)\n",
    "end_date = date(2021, 1, 15)\n",
    "dayrange = range((end_date - start_date).days + 1)\n",
    "\n",
    "for daycount in dayrange:\n",
    "    dt = start_date + timedelta(days=daycount)\n",
    "    datestr = dt.strftime('%Y-%m-%d')\n",
    "    fname = join(ARTICLES_DIR, datestr + '.csv')\n",
    "    if not exists(fname):\n",
    "        print(\"Downloading\", datestr)\n",
    "        all_results = []\n",
    "        my_params['from-date'] = datestr\n",
    "        my_params['to-date'] = datestr\n",
    "        current_page = 1\n",
    "        total_pages = 1\n",
    "        while current_page <= total_pages:\n",
    "            print(\"...page\", current_page)\n",
    "            my_params['page'] = current_page\n",
    "            resp = requests.get(API_ENDPOINT, my_params)\n",
    "            data = resp.json()\n",
    "            all_results.extend(data['response']['results'])\n",
    "            current_page += 1\n",
    "            total_pages = data['response']['pages']\n",
    "\n",
    "        with open(fname, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['id', 'webTitle', 'webUrl', 'sectionName', 'publicationDate'])\n",
    "            writer.writeheader()\n",
    "            for article in all_results:\n",
    "                writer.writerow({\n",
    "                    'id': article['id'],\n",
    "                    'webTitle': article['webTitle'],\n",
    "                    'webUrl': article['webUrl'],\n",
    "                    'sectionName': article['sectionName'],\n",
    "                    'publicationDate': article['webPublicationDate']\n",
    "                })\n",
    "        print(\"Writing to\", fname)\n",
    "    else:\n",
    "        print(\"File already exists:\", fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd128769-292c-45da-856b-1bfa4858dcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
